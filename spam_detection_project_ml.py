# -*- coding: utf-8 -*-
"""spam_detection_project_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SHSWlFPjDiZ-8wRtdlPe2lqNMyMJBlB

1 data clening

2 EDA

3 text preprocessing

4 model building

5 Evaluation

6 improvemment

7 website

8 deploy
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/spam.csv',encoding='latin-1')
df.sample(6)

df.shape

"""# data cleaning"""

df.info()

# last 3 columns have most null values and less non null i.e normalvalue so we are droping there three columns
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.sample(5)

df.isna().sum()

# renaming the columns
df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.sample(5)

# making target text in numbers
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

df['target']=encoder.fit_transform(df['target'])

df.head()

# missing value check
df.isna().sum()

# drop duplicates
df=df.drop_duplicates(keep='first')

"""
keep='first

‚ÄúI want to delete repeated rows, but keep the first time each one appears.‚Äù"""

df.shape

"""## 2 **EDA**"""

df.head()

df['target'].value_counts()

import matplotlib.pyplot as plt

plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show

# data inbalance more ham less spam

# now we are checking in message we will check how mnat alphabets how mabny words are being used
# numbes od sentanse in the essymess so using 'nltk

!pip install nltk

import nltk

nltk.download('punkt_tab')

df['num_charecters']=df['text'].apply(len)
#.apply(len) = give the lengh i.e number od chareter usef in text col

df.head()

# numbers of words
df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

# it will convets sentense into word and nltk .words_tokenize will do that in
# it wil and then it will count it eg inn a sence thire are 15 wirds so it will count each word sepeateely as the num words =15 as 15 words in the sentaence

df.head(3)

# numbes of sentenses
df['num_sentences']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

# it will work like sentence in mesge will cout sencet by sentence by tolen-sen
# on besis ofsentace we breck the mesage or divide into small parts eg 3 sencence in the message it will count as 3 only

df.head(3)

df[['num_charecters','num_words','num_sentences']].describe()

#ham
df[df['target'] == 0][['num_charecters','num_words','num_sentences']].describe()

#spam
df[df['target']==1][['num_charecters','num_words','num_sentences']].describe()

import seaborn as sns

plt.figure(figsize=(12,8))
sns.histplot(df[df['target']==0]['num_charecters'])
sns.histplot(df[df['target']==1]['num_charecters'])

plt.figure(figsize=(12,8))
sns.histplot(df[df['target']==0]['num_words'])
sns.histplot(df[df['target']==1]['num_words'])

sns.pairplot(df,hue='target')

df.info()

sns.heatmap(df[['target','num_charecters','num_words','num_sentences']].corr(),annot=True)

"""we will keep num_charector relationship with target it have corelation=0.38 more than 0.26,0.26 and other columns have very steon ewaltion with each other

## **Data** **Preprocessing**
 data preprocessing on text data

 lower case --- change text data into lower case


 tokenization --- breack data into small words

 removing special charectors



 removing stop words and punctuation
 the, is, in, at, of, for, and, a, to, if, on...

 üìå 2. Punctuation = Characters like . , ! ? ; : " ( )

stemming


Stemming is the process of reducing words to their root or base form, usually by chopping off prefixes or suffixes. runing -- run
"""

nltk.download('stopwords')
from nltk.corpus import stopwords
import string

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))


    return " ".join(y)

"""Example:    if i.isalnum():


Input:
"Hi! This is @chatgpt123."

Keep only alphanumeric characters:
"HiThisischatgpt123"
"""

transform_text('HI HOW ARE YOU 20% learning maching')

# stemming
from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()
ps.stem('loving')

#ps.stem('loving') = love

df['transform_text']=df['text'].apply(transform_text)

df.head()

# word cloud will make important  word big

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

span_wc=wc.generate(df[df['target']==1]['transform_text'].str.cat(sep=" "))

plt.figure(figsize=(12,6))
plt.imshow(span_wc)

ham_wc=wc.generate(df[df['target']==0]['transform_text'].str.cat(sep=" "))

plt.figure(figsize=(12,6))
plt.imshow(ham_wc)

# top 30 wors of ham and spam
spam_corpus=[]
for msg in df[df['target']==1]['transform_text'].tolist():
   for word in msg.split():
        spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0])
plt.xticks(rotation='vertical')
plt.show()

# top 30 wors of ham
ham_corpus=[]
for msg in df[df['target']==0]['transform_text'].tolist():
   for word in msg.split():
        ham_corpus.append(word)

len(ham_corpus)

from collections import Counter
sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0])
plt.xticks(rotation='vertical')
plt.show()

# on textual data naive baysed work good than other model

"""# **model** **building**"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

cv=CountVectorizer()
tfidf=TfidfVectorizer()

# x=cv.fit_transform(df['transform_text']).toarray()
x=tfidf.fit_transform(df['transform_text']).toarray()

x.shape

y=df['target'].values

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
# as we dont know which nave bayes type should use here

from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(x_train,y_train)
y_pred1=gnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(x_train,y_train)

y_pred2=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))  # data is inbalance so we  will
print(precision_score(y_test,y_pred2))    # depend on precision score

bnb.fit(x_train,y_train)

y_pred3=bnb.predict(x_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

"""# tfidf=TfidfVectorizer() now checking this and find which give better presion score"""

tfidf=TfidfVectorizer()

x=tfidf.fit_transform(df['transform_text']).toarray()

y=df['target'].values

gnb.fit(x_train,y_train)
y_pred1=gnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(x_train,y_train)

y_pred2=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))  # data is inbalance so we  will
print(precision_score(y_test,y_pred2))    # depend on precision score
# here presion score is perfect

bnb.fit(x_train,y_train)

y_pred3=bnb.predict(x_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

""" now here we see thta precision score is more imp here as the dat is in balce so we will depend on presion data  more so

here we notice TfidfVectorizer score of MultinomialNB precison score is 1.0 is perfect so we will take tfidvectorizer as answer
"""

# ifidf mnb selected

# saving model

import pickle

pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))